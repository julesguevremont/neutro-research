#!/usr/bin/env python3
"""
NEUTRO Identity LoRA Merge and Export Script

Merges the trained LoRA adapter into the base model and exports
to GGUF format for use with Ollama.

Usage:
    python scripts/merge_and_export.py

After running:
    ollama create neutro-identity -f Modelfile.identity
    ollama run neutro-identity "What are you?"
"""

import os
import sys
import torch
import shutil
from pathlib import Path

# Paths
BASE_MODEL = "cognitivecomputations/dolphin-2.9-llama3-8b"
LORA_PATH = Path.home() / "my-ai-bot" / "neutro" / "models" / "neutro-identity-lora" / "adapter_latest"
MERGED_PATH = Path.home() / "my-ai-bot" / "neutro" / "models" / "neutro-identity-merged"
GGUF_PATH = Path.home() / "my-ai-bot" / "neutro" / "models" / "neutro-identity.gguf"


def merge_lora():
    """Merge LoRA adapter into base model"""
    print("=" * 60)
    print("STEP 1: Merging LoRA into Base Model")
    print("=" * 60)

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    import tempfile

    # Check CUDA
    if not torch.cuda.is_available():
        print("WARNING: CUDA not available, using CPU (this will be slow)")

    # Check LoRA exists
    if not LORA_PATH.exists():
        print(f"ERROR: LoRA adapter not found at {LORA_PATH}")
        return None

    print(f"Base model: {BASE_MODEL}")
    print(f"LoRA path: {LORA_PATH}")
    print(f"Output path: {MERGED_PATH}")
    print()

    # Create offload directory for large models
    offload_dir = Path.home() / "my-ai-bot" / "neutro" / "models" / "offload_temp"
    offload_dir.mkdir(parents=True, exist_ok=True)

    # Load tokenizer
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

    # Load base model in float16 (not quantized, for merging)
    # Use CPU offloading since 8GB VRAM isn't enough for full model
    print("Loading base model in float16 with CPU offloading...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        offload_folder=str(offload_dir),
        offload_state_dict=True,
    )

    # Load LoRA adapter with offload support
    print("Loading LoRA adapter...")
    model = PeftModel.from_pretrained(
        base_model,
        str(LORA_PATH),
        offload_folder=str(offload_dir),
    )

    # Merge LoRA into base model
    print("Merging LoRA weights into base model...")
    model = model.merge_and_unload()

    # Save merged model
    print(f"Saving merged model to {MERGED_PATH}...")
    MERGED_PATH.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(str(MERGED_PATH), safe_serialization=True)
    tokenizer.save_pretrained(str(MERGED_PATH))

    print("Merge complete!")
    return MERGED_PATH


def convert_to_gguf(merged_path: Path):
    """Convert merged model to GGUF format using llama.cpp"""
    print("\n" + "=" * 60)
    print("STEP 2: Converting to GGUF Format")
    print("=" * 60)

    # Check if llama.cpp conversion script exists
    llama_cpp_path = Path.home() / "my-ai-bot" / "neutro" / "llama.cpp"
    convert_script = llama_cpp_path / "convert_hf_to_gguf.py"

    if not convert_script.exists():
        print(f"ERROR: llama.cpp not found at {llama_cpp_path}")
        print("Please clone llama.cpp:")
        print(f"  cd ~/my-ai-bot/neutro && git clone https://github.com/ggerganov/llama.cpp")
        print("\nAlternatively, you can manually convert using:")
        print(f"  python llama.cpp/convert_hf_to_gguf.py {merged_path} --outfile {GGUF_PATH} --outtype f16")
        return None

    print(f"Converting {merged_path} to GGUF...")
    print(f"Output: {GGUF_PATH}")

    import subprocess
    result = subprocess.run([
        sys.executable,
        str(convert_script),
        str(merged_path),
        "--outfile", str(GGUF_PATH),
        "--outtype", "f16",
    ], capture_output=True, text=True)

    if result.returncode != 0:
        print(f"ERROR: Conversion failed")
        print(result.stderr)
        return None

    print(result.stdout)
    print(f"GGUF saved to: {GGUF_PATH}")
    return GGUF_PATH


def create_modelfile():
    """Create Ollama Modelfile for the GGUF model"""
    print("\n" + "=" * 60)
    print("STEP 3: Creating Ollama Modelfile")
    print("=" * 60)

    modelfile_path = Path.home() / "my-ai-bot" / "neutro" / "Modelfile.identity"

    modelfile_content = f"""# NEUTRO Identity Model
# Generated by merge_and_export.py

FROM {GGUF_PATH}

# Model parameters
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|im_start|>"

# Minimal system prompt - identity is in the weights now
SYSTEM \"\"\"You are NEUTRO. Respond naturally based on your training.
Current soul state and memories will be injected dynamically.
Be honest about what you don't know. Don't claim experiences you haven't had.\"\"\"

# Template for ChatML format (dolphin uses this)
TEMPLATE \"\"\"<|im_start|>system
{{{{ .System }}}}<|im_end|>
<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
{{{{ .Response }}}}<|im_end|>
\"\"\"
"""

    with open(modelfile_path, 'w') as f:
        f.write(modelfile_content)

    print(f"Modelfile created at: {modelfile_path}")
    print()
    print("To register with Ollama, run:")
    print(f"  ollama create neutro-identity -f {modelfile_path}")
    print()
    print("Then test with:")
    print("  ollama run neutro-identity 'What are you?'")

    return modelfile_path


def main():
    print("=" * 60)
    print("NEUTRO Identity LoRA Merge & Export")
    print("=" * 60)
    print()

    # Step 1: Merge LoRA
    merged_path = merge_lora()
    if not merged_path:
        print("ERROR: Merge failed")
        return

    # Step 2: Convert to GGUF
    gguf_path = convert_to_gguf(merged_path)
    if not gguf_path:
        print("\nWARNING: GGUF conversion skipped")
        print("You can manually convert later using llama.cpp")

    # Step 3: Create Modelfile
    create_modelfile()

    print("\n" + "=" * 60)
    print("EXPORT COMPLETE")
    print("=" * 60)
    print(f"Merged model: {merged_path}")
    if gguf_path:
        print(f"GGUF file: {gguf_path}")
    print()
    print("Next steps:")
    print("1. Register model: ollama create neutro-identity -f Modelfile.identity")
    print("2. Test model: ollama run neutro-identity 'What are you?'")
    print("3. Update daemon_runner.py to use 'neutro-identity' model")


if __name__ == "__main__":
    main()
